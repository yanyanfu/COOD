# Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection

## Introduction
In this work, We proposed two multi-modal OOD detection methods for code related pretrained ML models; namely unsupervised COOD and weakly-supervised COOD+. The COOD merely leveraged unsupervised contrastive learning to identify OOD samples. As an extension of COOD, COOD+ combined contrastive learning and a binary classifier for OOD detection using a small number of labelled OOD samples. To reap the benefits of these two modules, we also devised a new scoring metric to fuse their prediction results. 

## Results
Given that all outliers were randomly selected, we present the average experimental outcomes across **five random seeds** of the test dataset to ensure evaluation reliability and reproducibility. As indicated in Tables 2 and 3, our unsupervised COOD model demonstrates superior performance compared to the unsupervised baseline models **with statistical significance** (Wilcoxon‚Äôs paired test ùëù < 0.05). Similarly, our weakly-supervised COOD+ model exhibits superior performance compared to the weakly-supervised baseline models **with statistical significance** (Wilcoxon‚Äôs paired test ùëù < 0.05). Wilcoxon‚Äôs paired tests were conducted between COOD/COOD+ and unsupervised/weakly-supervised baseline models based on the OOD scores generated by these models.

It is important to note that **the encoder backbone of baseline models 1-3 (i.e., SCL, MCL, ECE) is also GraphCodeBERT**, ensuring a fair comparison with our COOD/COOD+ models. While each of the baseline models 1-3 relies on an encoder backbone to generate probabilities for OOD scoring, baseline models 4-5 (i.e., CuBERT, 2P-CuBERT) are specifically designed for neural bug detection, obviating the need for another encoder backbone for OOD detection.

Furthermore, Table 1 presents the initial version of dataset statistics used for training and evaluating our COOD+. However, in subsequent experiments, we updated the OOD scenarios to reflect more realistic situations, so the dataset statistics is a bit different. Unfortunately, we inadvertently failed to update the numbers in the tables for our final paper submission. In fact, the results numbers shown in Tables 2, 3, 4, and 5 in our paper are based on the following correct/latest dataset statistics for training and evaluation of our weakly-supervised COOD+ and baseline models.

<img src="figs/dataset.png" alt="visual results" width="480">

The results for hyperparameter tuning are shown below

<img src="figs/hyper.png" alt="visual results" width="560">


## Training Prerequisites
- python 3.9
- pytorch 1.8.0
- torchvision 0.9.0

## Dataset & Evaluation benchmark

To acquire the training and testing datasets, please first download the datasets used for OOD data generation, including the datasets constructed from StackOverflow for code search and the datasets constructed from CSN by injecting variable misuse bugs (single token-based). Then, you can run the preprocess.py and preprocess.ipynb under each subfolder of the data folder. Consequently, the OOD data would be extracted automatically based on the dataloader.py file when you train and evaluate the models.

## Reproduce the results

The parameter values in config_java.yaml and config_python.yaml can be changed to adapt to different settings. Our trained COOD and COOD+ model checkpoints are available  [here](https://doi.org/10.5281/zenodo.10455811) anonymously for results reproduction.


```bash
git clone https://anonymous.4open.science/r/COOD-4EA6
cd COOD
pip install -r requirements.txt
```

To train the model
```bash
python run.py --config config_java.yaml
```

To test the model under OOD detection setting
```bash
python run.py --config config_java.yaml --test_baseline_metrics
```

To test the model on the main code understanding task (code search) with COOD+ auxiliary
```bash
python run.py --config config_java.yaml  --test_main_task
```
