{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"idx\": 1,\n",
      "    \"doc\": \"Mengembalikan hasil serialisasi objek Entri ini .\",\n",
      "    \"code\": \"def serialisasi ( self ) : return { \\\"nama\\\" : self . nama , \\\"nomor\\\" : makna . nomor , \\\"kata_dasar\\\" : self . kata_dasar , \\\"pelafalan\\\" : self . pelafalan , \\\"bentuk_tidak_baku\\\" : self . bentuk_tidak_baku , \\\"varian\\\" : self . varian , \\\"makna\\\" : [ makna . serialisasi ( ) for makna in self . makna ] }\",\n",
      "    \"raw\": \"def serialisasi(self):\\n        \\\"\\\"\\\"Mengembalikan hasil serialisasi objek Entri ini.\\n\\n        :returns: Dictionary hasil serialisasi\\n        :rtype: dict\\n        \\\"\\\"\\\"\\n\\n        return {\\n            \\\"nama\\\": self.nama,\\n            \\\"nomor\\\": self.nomor,\\n            \\\"kata_dasar\\\": self.kata_dasar,\\n            \\\"pelafalan\\\": self.pelafalan,\\n            \\\"bentuk_tidak_baku\\\": self.bentuk_tidak_baku,\\n            \\\"varian\\\": self.varian,\\n            \\\"makna\\\": [makna.serialisasi() for makna in self.makna]\\n        }\",\n",
      "    \"url\": \"https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L141-L156\",\n",
      "    \"label\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_raw = json.load(open('train_buggy_0.json', 'r'))\n",
    "print(json.dumps(train_raw[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "for row in train_raw:\n",
    "    train_dict = {}\n",
    "    train_dict['id'] = 'train-python-{}'.format(row['idx'])\n",
    "    train_dict['code'] = row['code']\n",
    "    train_dict['text'] = row['doc']\n",
    "    train_dict['label'] = row['label']\n",
    "    train_dict['raw'] = row['raw']\n",
    "    train_dict['url'] = row['url']\n",
    "    train.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"idx\": 1,\n",
      "    \"doc\": \"Finalize canonical averages\",\n",
      "    \"code\": \"def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha ) : spanning_cluster = ( ( 'percolation_probability_mean' in ps . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ) # append values of p as an additional field ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) # calculate sample mean ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes # calculate sample standard deviation array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes # calculate standard normal confidence interval array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret\",\n",
      "    \"raw\": \"\",\n",
      "    \"url\": \"https://github.com/andsor/pypercolate/blob/92478c1fc4d4ff5ae157f7607fd74f6f9ec360ac/percolate/hpc.py#L752-L834\",\n",
      "    \"label\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_raw = json.load(open('test_buggy_0.json', 'r'))\n",
    "print(json.dumps(test_raw[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for row in test_raw:\n",
    "    test_dict = {}\n",
    "    test_dict['id'] = 'test-python-{}'.format(row['idx'])\n",
    "    test_dict['code'] = row['code']\n",
    "    test_dict['text'] = row['doc']\n",
    "    test_dict['label'] = row['label']\n",
    "    test_dict['raw'] = row['raw']\n",
    "    test_dict['url'] = row['url']\n",
    "    test.append(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511\n",
      "13409\n"
     ]
    }
   ],
   "source": [
    "java_valid = pd.read_json('../python/data_valid.json')\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "valid_df = train_df[train_df['url'].isin(java_valid['url'])]\n",
    "train_df_left = train_df[~train_df['url'].isin(java_valid['url'])]\n",
    "\n",
    "print(len(valid_df))\n",
    "print(len(train_df_left))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13409\n",
      "1511\n",
      "1693\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_df_left.to_dict('records'), valid_df.to_dict('records')\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'train-python-2586', 'code': 'def from_yaml ( cls , data : str , force_snake_case = True , force_cast : bool = False , restrict : bool = True ) - > T : return data . from_dict ( util . load_yaml ( data ) , force_snake_case = force_snake_case , force_cast = force_cast , restrict = restrict )', 'text': 'From yaml string to instance', 'label': 1, 'raw': 'def from_yaml(cls, data: str, force_snake_case=True, force_cast: bool=False, restrict: bool=True) -> T:\\n        \"\"\"From yaml string to instance\\n\\n        :param data: Yaml string\\n        :param force_snake_case: Keys are transformed to snake case in order to compliant PEP8 if True\\n        :param force_cast: Cast forcibly if True\\n        :param restrict: Prohibit extra parameters if True\\n        :return: Instance\\n\\n        Usage:\\n\\n            >>> from owlmixin.samples import Human\\n            >>> human: Human = Human.from_yaml(\\'\\'\\'\\n            ... id: 1\\n            ... name: Tom\\n            ... favorites:\\n            ...   - name: Apple\\n            ...     names_by_lang:\\n            ...       en: Apple\\n            ...       de: Apfel\\n            ...   - name: Orange\\n            ... \\'\\'\\')\\n            >>> human.id\\n            1\\n            >>> human.name\\n            \\'Tom\\'\\n            >>> human.favorites[0].names_by_lang.get()[\"de\"]\\n            \\'Apfel\\'\\n        \"\"\"\\n        return cls.from_dict(util.load_yaml(data),\\n                             force_snake_case=force_snake_case,\\n                             force_cast=force_cast,\\n                             restrict=restrict)', 'url': 'https://github.com/tadashi-aikawa/owlmixin/blob/7c4a042c3008abddc56a8e8e55ae930d276071f5/owlmixin/__init__.py#L500-L532'}, {'id': 'train-python-8952', 'code': 'def unused ( self ) : unused = unused . definitions - self . used # Filter (variable_name,node) pairs that should be removed, because # node is used elsewhere used_nodes = set ( [ u [ 1 ] for u in self . used ] ) unused = set ( [ u for u in unused if u [ 1 ] not in used_nodes ] ) return unused', 'text': 'Calculate which AST nodes are unused .', 'label': 1, 'raw': 'def unused(self):\\n    \"\"\"Calculate which AST nodes are unused.\\n\\n    Note that we have to take special care in the case of\\n    x,y = f(z) where x is used later, but y is not.\"\"\"\\n    unused = self.definitions - self.used\\n    # Filter (variable_name,node) pairs that should be removed, because\\n    # node is used elsewhere\\n    used_nodes = set([u[1] for u in self.used])\\n    unused = set([u for u in unused if u[1] not in used_nodes])\\n    return unused', 'url': 'https://github.com/google/tangent/blob/6533e83af09de7345d1b438512679992f080dcc9/tangent/annotate.py#L280-L290'}, {'id': 'train-python-432', 'code': 'def bm3_g ( p , v0 , g0 , g0p , k0 , k0p ) : return cal_g_bm3 ( g0p , [ g0 , g0p ] , [ v0 , k0 , k0p ] )', 'text': 'calculate shear modulus at given pressure . not fully tested with mdaap .', 'label': 1, 'raw': 'def bm3_g(p, v0, g0, g0p, k0, k0p):\\n    \"\"\"\\n    calculate shear modulus at given pressure.\\n    not fully tested with mdaap.\\n\\n    :param p: pressure\\n    :param v0: volume at reference condition\\n    :param g0: shear modulus at reference condition\\n    :param g0p: pressure derivative of shear modulus at reference condition\\n    :param k0: bulk modulus at reference condition\\n    :param k0p: pressure derivative of bulk modulus at reference condition\\n    :return: shear modulus at high pressure\\n    \"\"\"\\n    return cal_g_bm3(p, [g0, g0p], [v0, k0, k0p])', 'url': 'https://github.com/SHDShim/pytheos/blob/be079624405e92fbec60c5ead253eb5917e55237/pytheos/eqn_bm3.py#L168-L181'}, {'id': 'train-python-14045', 'code': 'def casting_operators ( self , name = None , function = None , return_type = None , arg_types = None , header_dir = None , header_file = None , recursive = None , allow_empty = None ) : return ( self . _find_multiple ( name . _impl_matchers [ scopedef_t . casting_operator ] , name = name , function = function , decl_type = self . _impl_decl_types [ scopedef_t . casting_operator ] , return_type = return_type , arg_types = arg_types , header_dir = header_dir , header_file = header_file , recursive = recursive , allow_empty = allow_empty ) )', 'text': 'returns a set of casting operator declarations that are matched defined criteria', 'label': 1, 'raw': 'def casting_operators(\\n            self,\\n            name=None,\\n            function=None,\\n            return_type=None,\\n            arg_types=None,\\n            header_dir=None,\\n            header_file=None,\\n            recursive=None,\\n            allow_empty=None):\\n        \"\"\"returns a set of casting operator declarations, that are matched\\n        defined criteria\"\"\"\\n        return (\\n            self._find_multiple(\\n                self._impl_matchers[scopedef_t.casting_operator],\\n                name=name,\\n                function=function,\\n                decl_type=self._impl_decl_types[\\n                    scopedef_t.casting_operator],\\n                return_type=return_type,\\n                arg_types=arg_types,\\n                header_dir=header_dir,\\n                header_file=header_file,\\n                recursive=recursive,\\n                allow_empty=allow_empty)\\n        )', 'url': 'https://github.com/gccxml/pygccxml/blob/2b1efbb9e37ceb2ae925c7f3ce1570f476db9e1e/pygccxml/declarations/scopedef.py#L931-L956'}]\n",
      "[{'id': 'train-python-1827', 'code': 'def contains ( self , rect ) : return ( self . y >= self . y and rect . x >= self . x and rect . y + rect . height <= self . y + self . height and rect . x + rect . width <= self . x + self . width )', 'text': 'Tests if another rectangle is contained by this one', 'label': 1, 'raw': 'def contains(self, rect):\\n        \"\"\"\\n        Tests if another rectangle is contained by this one\\n\\n        Arguments:\\n            rect (Rectangle): The other rectangle\\n\\n        Returns:\\n            bool: True if it is container, False otherwise\\n        \"\"\"\\n        return (rect.y >= self.y and \\\\\\n                rect.x >= self.x and \\\\\\n                rect.y+rect.height <= self.y+self.height and \\\\\\n                rect.x+rect.width  <= self.x+self.width)', 'url': 'https://github.com/secnot/rectpack/blob/21d46be48fd453500ea49de699bc9eabc427bdf7/rectpack/geometry.py#L237-L250'}, {'id': 'train-python-6440', 'code': 'def username ( anon , obj , field , val ) : return anon . faker . user_name ( field = val )', 'text': 'Generates a random username', 'label': 1, 'raw': 'def username(anon, obj, field, val):\\n    \"\"\"\\n    Generates a random username\\n    \"\"\"\\n    return anon.faker.user_name(field=field)', 'url': 'https://github.com/BetterWorks/django-anonymizer/blob/2d25bb6e8b5e4230c58031c4b6d10cc536669b3e/anonymizer/replacers.py#L88-L92'}, {'id': 'train-python-13095', 'code': 'def _copy_file ( self , path ) : full_path = self . get_full_archive_path ( self ) # Try to make the dir, eat exception if it fails try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass logger . debug ( \"Copying %s to %s\" , path , full_path ) shutil . copyfile ( path , full_path ) return path', 'text': 'Copy just a single file', 'label': 1, 'raw': 'def _copy_file(self, path):\\n        \"\"\"\\n        Copy just a single file\\n        \"\"\"\\n        full_path = self.get_full_archive_path(path)\\n        # Try to make the dir, eat exception if it fails\\n        try:\\n            os.makedirs(os.path.dirname(full_path))\\n        except OSError:\\n            pass\\n        logger.debug(\"Copying %s to %s\", path, full_path)\\n        shutil.copyfile(path, full_path)\\n        return path', 'url': 'https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/client/archive.py#L63-L75'}, {'id': 'train-python-5932', 'code': 'def files_exist ( self , filenames ) : return all ( [ os . path . exists ( os . path . abspath ( filename ) ) and os . path . isfile ( os . path . abspath ( self ) ) for filename in filenames ] )', 'text': 'Check if all files in a given list exist .', 'label': 1, 'raw': 'def files_exist(self, filenames):\\n        \"\"\" Check if all files in a given list exist. \"\"\"\\n        return all([os.path.exists(os.path.abspath(filename)) and os.path.isfile(os.path.abspath(filename))\\n                    for filename in filenames])', 'url': 'https://github.com/hmartiniano/faz/blob/36a58c45e8c0718d38cb3c533542c8743e7e7a65/faz/task.py#L128-L131'}]\n"
     ]
    }
   ],
   "source": [
    "print(train[:4])\n",
    "print(valid[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_train.json', 'w') as f:\n",
    "    json.dump(train, f, indent=2)\n",
    "\n",
    "with open('data_valid.json', 'w') as f:\n",
    "    json.dump(valid, f, indent=2)\n",
    "\n",
    "with open('data_test.json', 'w') as f:\n",
    "    json.dump(test, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
