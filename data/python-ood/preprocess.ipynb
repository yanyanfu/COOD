{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"idx\": 101,\n",
      "    \"doc\": \"Extract the loss categories and the tags of the exposure . Use it as / extract / exposure_metadata\",\n",
      "    \"code\": \"def extract_exposure_metadata ( dstore , what ) : dic = { } dic1 , dic2 = dstore [ 'assetcol/tagcol' ] . __toh5__ ( ) dic . update ( dic1 ) dic . update ( dic2 ) if 'asset_risk' in dstore : dic [ 'multi_risk' ] = sorted ( set ( dstore [ 'asset_risk' ] . dtype . names ) - set ( dstore [ 'assetcol/array' ] . dtype . names ) ) names = [ name for name in dstore [ 'assetcol/array' ] . dtype . names if name . startswith ( ( 'value-' , 'number' , 'occupants_' ) ) and not name . endswith ( '_None' ) ] return ArrayWrapper ( numpy . array ( names ) , dic )\",\n",
      "    \"raw\": \"def extract_exposure_metadata(dstore, what):\\n    \\\"\\\"\\\"\\n    Extract the loss categories and the tags of the exposure.\\n    Use it as /extract/exposure_metadata\\n    \\\"\\\"\\\"\\n    dic = {}\\n    dic1, dic2 = dstore['assetcol/tagcol'].__toh5__()\\n    dic.update(dic1)\\n    dic.update(dic2)\\n    if 'asset_risk' in dstore:\\n        dic['multi_risk'] = sorted(\\n            set(dstore['asset_risk'].dtype.names) -\\n            set(dstore['assetcol/array'].dtype.names))\\n    names = [name for name in dstore['assetcol/array'].dtype.names\\n             if name.startswith(('value-', 'number', 'occupants_'))\\n             and not name.endswith('_None')]\\n    return ArrayWrapper(numpy.array(names), dic)\",\n",
      "    \"label\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_raw = json.load(open('train_ood_codesearchnet.json', 'r'))\n",
    "print(json.dumps(train_raw[100], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251820/251820 [00:04<00:00, 56697.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from bpe import BpeVocabulary\n",
    "\n",
    "code_vocab_counter = Counter()\n",
    "text_vocab_counter = Counter()\n",
    "\n",
    "train_id = json.load(open('../python/train_codesearchnet_0.json', 'r'))\n",
    "\n",
    "for row in tqdm(train_id):\n",
    "    code = row['code'].split()\n",
    "    text = row['doc'].split()\n",
    "    code_vocab_counter.update(code)\n",
    "    text_vocab_counter.update(text)\n",
    "\n",
    "code_bpe_vocab = BpeVocabulary(vocab_size=10000, pct_bpe=0.5)\n",
    "text_bpe_vocab = BpeVocabulary(vocab_size=10000, pct_bpe=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067300/1067300 [01:10<00:00, 15211.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72846/72846 [00:02<00:00, 25784.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "code_bpe_vocab.fit(code_vocab_counter)\n",
    "text_bpe_vocab.fit(text_vocab_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# check if text contains special tokens like urls, emails, etc.\n",
    "def contains_special_token(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.like_url or token.like_email:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/160358 [00:00<1:00:04, 44.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160358/160358 [42:10<00:00, 63.36it/s] \n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "\n",
    "for i, row in enumerate(tqdm(train_raw)):\n",
    "    train_dict = {}\n",
    "    train_dict['id'] = 'train-python-ood-{}'.format(row['idx'])\n",
    "    train_dict['code'] = row['code']\n",
    "    train_dict['text'] = row['doc']\n",
    "    train_dict['label'] = row['label']\n",
    "    train_dict['raw'] = row['raw']\n",
    "\n",
    "    if len(tokenize(train_dict['text'])) <= 3 or contains_special_token(train_dict['text']):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        lang = detect(train_dict['text'])\n",
    "        if lang != 'en':\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    train_dict['code_bpe'] = ' '.join(code_bpe_vocab.tokenize(row['code'].split()))\n",
    "    train_dict['text_bpe'] = ' '.join(text_bpe_vocab.tokenize(row['doc'].split()))\n",
    "\n",
    "    train.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136804\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"idx\": 1,\n",
      "    \"doc\": \"Extracts video ID from URL .\",\n",
      "    \"code\": \"def get_vid_from_url ( url ) : return match1 ( url , r'youtu\\\\.be/([^?/]+)' ) or match1 ( url , r'youtube\\\\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\\\\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\\\\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' )\",\n",
      "    \"raw\": \"def get_vid_from_url(url):\\n        \\\"\\\"\\\"Extracts video ID from URL.\\n        \\\"\\\"\\\"\\n        return match1(url, r'youtu\\\\.be/([^?/]+)') or \\\\\\n          match1(url, r'youtube\\\\.com/embed/([^/?]+)') or \\\\\\n          match1(url, r'youtube\\\\.com/v/([^/?]+)') or \\\\\\n          match1(url, r'youtube\\\\.com/watch/([^/?]+)') or \\\\\\n          parse_query_param(url, 'v') or \\\\\\n          parse_query_param(parse_query_param(url, 'u'), 'v')\",\n",
      "    \"label\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_raw = json.load(open('valid_ood_codesearchnet.json', 'r'))\n",
    "print(json.dumps(test_raw[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for row in test_raw:\n",
    "    test_dict = {}\n",
    "    test_dict['id'] = 'test-python-ood-{}'.format(row['idx'])\n",
    "    test_dict['code'] = row['code']\n",
    "    test_dict['text'] = row['doc']\n",
    "    test_dict['label'] = row['label']\n",
    "    test_dict['raw'] = row['raw']\n",
    "    test_dict['code_bpe'] = ' '.join(code_bpe_vocab.tokenize(row['code'].split()))\n",
    "    test_dict['text_bpe'] = ' '.join(text_bpe_vocab.tokenize(row['doc'].split()))\n",
    "\n",
    "    if len(tokenize(train_dict['text'])) <= 3 or contains_special_token(train_dict['text']):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        lang = detect(train_dict['text'])\n",
    "        if lang != 'en':\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    test.append(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "\n",
    "valid_size = int(len(train) * 0.1)\n",
    "valid = train[:valid_size]\n",
    "train = train[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123124\n",
      "13680\n",
      "7258\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_train.json', 'w') as f:\n",
    "    json.dump(train, f, indent=2)\n",
    "\n",
    "with open('data_valid.json', 'w') as f:\n",
    "    json.dump(valid, f, indent=2)\n",
    "\n",
    "with open('data_test.json', 'w') as f:\n",
    "    json.dump(test, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"id\": \"train-python-ood-7657\",\n",
      "        \"code\": \"def sg_hinge ( tensor , opt ) : assert opt . target is not None , 'target is mandatory.' # default margin opt += tf . sg_opt ( margin = 1 ) # reshape target shape = tensor . get_shape ( ) . as_list ( ) broadcast_shape = [ - 1 ] + [ 1 ] * ( len ( shape ) - 2 ) + [ shape [ - 1 ] ] target = tf . cast ( tf . reshape ( opt . target , broadcast_shape ) , tf . sg_floatx ) # hinge loss out = tf . identity ( tf . maximum ( opt . margin - target * tensor , 0 ) , 'hinge' ) # add summary tf . sg_summary_loss ( out , name = opt . name ) return out\",\n",
      "        \"text\": \"r Returns hinge loss between tensor and target . Args : tensor : A Tensor . opt : target : A Tensor . Labels . margin : An int . Maximum margin . Default is 1 . name : A string . A name to display in the tensor board web UI . Returns : A Tensor . For example tensor = [[ 30 10 40 ] [ 13 30 42 ]] target = [[ 0 0 1 ] [ 0 1 0 ]] tensor . sg_hinge ( target = target one_hot = True ) = > [[ 1 . 1 . 0 . ] [ 1 . 0 . 1 . ]]\",\n",
      "        \"label\": 1,\n",
      "        \"raw\": \"def sg_hinge(tensor, opt):\\n    r\\\"\\\"\\\"Returns hinge loss between `tensor` and `target`.\\n    \\n    Args:\\n      tensor: A `Tensor`.\\n      opt:\\n        target: A `Tensor`. Labels.\\n        margin: An int. Maximum margin. Default is 1.\\n        name: A `string`. A name to display in the tensor board web UI.\\n      \\n    Returns:\\n      A `Tensor`.\\n    \\n    For example,\\n    \\n    ```\\n    tensor = [[30, 10, 40], [13, 30, 42]]\\n    target = [[0, 0, 1], [0, 1, 0]]\\n    tensor.sg_hinge(target=target, one_hot=True) =>     [[ 1.  1.  0.]\\n                                                         [ 1.  0.  1.]]\\n    ```\\n    \\\"\\\"\\\"\\n    assert opt.target is not None, 'target is mandatory.'\\n\\n    # default margin\\n    opt += tf.sg_opt(margin=1)\\n\\n    # reshape target\\n    shape = tensor.get_shape().as_list()\\n    broadcast_shape = [-1] + [1] * (len(shape) - 2) + [shape[-1]]\\n    target = tf.cast(tf.reshape(opt.target, broadcast_shape), tf.sg_floatx)\\n    \\n    # hinge loss\\n    out = tf.identity(tf.maximum(opt.margin - target * tensor, 0), 'hinge')\\n\\n    # add summary\\n    tf.sg_summary_loss(out, name=opt.name)\\n\\n    return out\",\n",
      "        \"code_bpe\": \"def <sow> sg _hi nge <eow> ( tensor , opt ) : assert opt . target is not None , <sow> 't arget <eow> is <sow> mand ator y. ' <eow> # default margin opt += tf . <sow> sg _opt <eow> ( margin = 1 ) # reshape target shape = tensor . get_shape ( ) . as_list ( ) <sow> br oad cas t_s hape <eow> = [ - 1 ] + [ 1 ] * ( len ( shape ) - 2 ) + [ shape [ - 1 ] ] target = tf . cast ( tf . reshape ( opt . target , <sow> br oad cas t_s hape <eow> ) , tf . <sow> sg _fl oat x <eow> ) # <sow> hing e <eow> loss out = tf . identity ( tf . maximum ( opt . margin - target * tensor , 0 ) , <sow> 'h ing e' <eow> ) # add summary tf . <sow> sg _su mma ry_ los s <eow> ( out , name = opt . name ) return out\",\n",
      "        \"text_bpe\": \"r Returns <sow> hing e <eow> loss between tensor and target . <sow> Ar gs <eow> <sow> <unk> <eow> tensor <sow> <unk> <eow> A Tensor . <sow> opt <eow> <sow> <unk> <eow> target <sow> <unk> <eow> A Tensor . <sow> La bel s <eow> . margin <sow> <unk> <eow> An int . Maximum margin . Default is 1 . name <sow> <unk> <eow> A string . A name to display in the tensor board web UI . Returns <sow> <unk> <eow> A Tensor . For example tensor = <sow> <unk> <unk> <eow> <sow> 3 0 <eow> 10 <sow> 4 0 <eow> <sow> <unk> <eow> [ <sow> 1 3 <eow> <sow> 3 0 <eow> <sow> 4 2 <eow> <sow> <unk> <unk> <eow> target = <sow> <unk> <unk> <eow> 0 0 1 <sow> <unk> <eow> [ 0 1 0 <sow> <unk> <unk> <eow> tensor . <sow> sg _h ing e <eow> ( target = target <sow> one _h ot <eow> = True ) = > <sow> <unk> <unk> <eow> 1 . 1 . 0 . <sow> <unk> <eow> [ 1 . 0 . 1 . <sow> <unk> <unk> <eow>\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"train-python-ood-46246\",\n",
      "        \"code\": \"def get_origin ( tp ) : if NEW_TYPING : if isinstance ( tp , _GenericAlias ) : return tp . __origin__ if tp . __origin__ is not ClassVar else None if tp is Generic : return Generic return None if isinstance ( tp , GenericMeta ) : return _gorg ( tp ) if is_union_type ( tp ) : return Union return None\",\n",
      "        \"text\": \"Get the unsubscripted version of a type . Supports generic types Union Callable and Tuple . Returns None for unsupported types . Examples ::\",\n",
      "        \"label\": 1,\n",
      "        \"raw\": \"def get_origin(tp):\\n    \\\"\\\"\\\"Get the unsubscripted version of a type. Supports generic types, Union,\\n    Callable, and Tuple. Returns None for unsupported types. Examples::\\n\\n        get_origin(int) == None\\n        get_origin(ClassVar[int]) == None\\n        get_origin(Generic) == Generic\\n        get_origin(Generic[T]) == Generic\\n        get_origin(Union[T, int]) == Union\\n        get_origin(List[Tuple[T, T]][int]) == list  # List prior to Python 3.7\\n    \\\"\\\"\\\"\\n    if NEW_TYPING:\\n        if isinstance(tp, _GenericAlias):\\n            return tp.__origin__ if tp.__origin__ is not ClassVar else None\\n        if tp is Generic:\\n            return Generic\\n        return None\\n    if isinstance(tp, GenericMeta):\\n        return _gorg(tp)\\n    if is_union_type(tp):\\n        return Union\\n\\n    return None\",\n",
      "        \"code_bpe\": \"def <sow> get_ orig in <eow> ( tp ) : if <sow> NE W _T YP ING <eow> : if isinstance ( tp , <sow> _ Ge ner ic Al ias <eow> ) : return tp . <sow> __ orig in_ _ <eow> if tp . <sow> __ orig in_ _ <eow> is not <sow> Cl ass Va r <eow> else None if tp is <sow> Ge ner ic <eow> : return <sow> Ge ner ic <eow> return None if isinstance ( tp , <sow> Ge ner ic Me ta <eow> ) : return <sow> _g or g <eow> ( tp ) if <sow> is_ uni on_t ype <eow> ( tp ) : return Union return None\",\n",
      "        \"text_bpe\": \"Get the <sow> uns ubs cript ed <eow> version of a type . Supports generic types Union <sow> Cal lable <eow> and Tuple . Returns None for <sow> uns upp orte d <eow> types . <sow> Ex ample s <eow> <sow> <unk> <unk> <eow>\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(valid[:2], indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6b883c92240247a61f0a90bd9fcab4b7e88d74efaf3f0a2714ac8f9d9a63ad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
